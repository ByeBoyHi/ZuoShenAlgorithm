Q1：

1. 最多使用1G内存：查询对应的数字是否有出现过————位图。
不是所有语言都有Bit类型。
可以用位图：一个int是32位，那么10长度的int就可以保存320位的bit位，也就是2^320个数据。
2^32/8(byte) ≈ 500M。
出现的地方描黑，从而我们查询需要的数据是否有出现过的时候，只需要去看对应的位置是否有描黑。
注意：MAX_VALUE大概是20亿

2. 最多使用10M内存（或者使用3KB空间）：需求更改————找一个没出现过的数字（Q2）
3KB/4≈512（一个数字是4字节）
然后对 2^32-1均分成512份，也就是 int[512]：2^32/2^9 = 8388608
对每个数字除以838608，得到的商就是它的位置。
然后在对应位置的值++，因为有40亿个数，那么一定有一个位置的词频不够8388608。
那么就知道这个数字来自哪个范围了，然后在这个范围上的数字，继续分成512份，继续过40亿个数字。
然后没有在我们上面找的范围的数字不要，在这个范围的，继续细分。
周而复始，从而找到那个没有出现的数字。
这就类似于词频统计，利用了词频统计一定不够的情况，逐渐找到它属于哪个范围。

3. 需求升级————假设只能申请有限几个变量：怎么判断哪个数字是没出现过的
对2^32-1二分，然后对40亿个数字进行两边的词频统计。
左侧满的，就是=2^16次方，右侧同理。
哪边不满，继续二分。
从而找到那个不满的里面的一个没有出现的数字。
最多二分5次。（2^5==32）
也就是用变量对两边的数字出现的次数进行计数，使用两个变量就可以记录42亿数据。
出现满的现象，直接跳出，对另一部分进行二分。

Q2：有一个100亿个URL的大文件，假设每个URL占64B，请找出其中所有重复的URL。
1. 布隆过滤去
2. 哈希函数均匀分流
补充：其搜索公司一天的用户搜索词汇是海量的（百亿数据量），请设计一种求出每天热门Top100词汇的可行方法。
用一个若干个堆，然后把每个词汇根据哈希分流的方式放进指定的堆里面，在堆里面进行词频计数
然后再用一个总堆，把这些堆的堆顶放进去，然后弹出总堆堆顶元素放进Top序列
再把弹出的堆顶之前所在的堆的新的堆顶再放进去
如此往复，直到Top放满100

Q3：
1. 位图进行计数：用两个位来表示一个数字出现的状态 ———— 00 01 10 11，我们就知道了他出现次数的0~3+的范围
    我们取10即可。以此类推我们可以得到出现k次的数字。
2. 哈希表进行种类分类，1GB进行一个字节8bit计算，得出可以分出多少个小文件，然后对每个数据放入对应的小文件
   在小文件对每个数字出现的次数进行计数，所有小文件出现两次的数字就是我们最后的答案
   （多个小文件占用的硬盘空间，使用Hash表的时候占用的内存空间）
补充：最多使用10M的内存，怎么找到40亿个数字的中位数
假设只有10KB可以用，那么10KB/4B≈2500，我们定义一个2048长度的int数组
然后对每个数字进行2048份的分流，也就是0~2048在arr[0], 2048~2048*2-1在arr[1]，以此类推
然后从arr[0]挨个往后加，当超过了20亿后，那么中位数一定在我们最后加的那个位置上
然后对这个位置上面的数字用相同的方式再继续分流处理。
假设：arr[0]+..+arr[40]=18亿数据，然后arr[41]=5亿。
那么我们需要在arr[41]上面找第2亿个数据，就可以和前面的18亿加起来，正好20亿，是中位数。
然后如果还是差一点儿，就不断重复找差值。
（对每个数字直接计算哈希值，然后除以2048，就得到他的数组格子）


Mapper Reduce：
大问题拆成小问题，叫Mapper，小问题合成原问题，叫Reduce。


加题：（腾讯面试题）有一个10G文件，里面是int类型，无序的数据（占用的硬盘空间）；
现在提供5G的内存空间，对10G的数据进行处理，输出一个10G的有序的数据。

思路一：
考虑用小根堆，统计一个数字和这个数字的词频，小根堆根据数字排序。
一条数据需要的内存大小：数字+词频=int+int=8B，假设考虑上堆的索引内存，一条数据总共需要16B。
那么5G的内存可以放下的数据条大小是：5G/16B≈5*2^26，假设建立2^27条数据。
对于10G数据里面的每个数字，他们的范围都是int，即-2^31~2^31-1这个范围。
总共是2^32的范围，就相当于有2^5个2^27的数据条。
那么我们建立一个小根堆，存储2^27的数据大小，然后过10G的数据。
第一次：数据在 -2^31 ~ -2^31+2^27-1 这个范围的数据放进去，然后统计词频建堆，一遍结束后放进新的硬盘空间。
第二次：数据在 -2^31+2^27 ~ -2^31+2^27*2-1  这个范围的数据放进去，统计词频建堆，再挨个放进硬盘
直到2^5次，所有数据统计完毕，都访问了，数据也就排序完毕。

思路二：
建立大根堆，和一个记录变量Y。
假设这个大根堆存放500W个数据，用来存10G数据里面的最小的500W个数据。
当数据没有装满500W个的时候，就不断的装进去。
当装满500W个的时候，堆顶就是这500W个数据的最大的元素。
这时候再来元素的时候，判断这个元素是否的大于堆顶，如果大于，那么不能进堆，
因为他不在最小的500W个数据的一个。
否则，就把堆顶元素弹出，让他入堆，然后调整堆。
过完10G数据后，就可以选出最终的500W条最小数据，然后把这些数据从后往前的放入硬盘空间，也就是升序。
这一次更新完数据后，我们的记录变量Y就更新位这次数据的最大值，
在下次选500W的数据的时候，就跳过所有小于等于Y的数据。
（这里的数据条依然是：数据 -- 词频）
